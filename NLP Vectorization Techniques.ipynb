{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e39bff",
   "metadata": {},
   "source": [
    "# Introduction to NLP in Python\n",
    "## Vectorization\n",
    "\n",
    "### Bag-of-Words Model (BOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0769fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5289f",
   "metadata": {},
   "source": [
    "Create an instance of the CountVectorizer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c684896",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3ca6f",
   "metadata": {},
   "source": [
    "Load your text data into a list or an array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57875cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = [\n",
    "    \"Supervised learning is a type of machine learning\",\n",
    "    \"Unsupervised learning is another type of machine learning\",\n",
    "    \"Machine learning algorithms can be used for NLP\",\n",
    "    \"NLP is a subfield of machine learning\",\n",
    "    \"Sentiment analysis is a type of NLP application\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6256d9b4",
   "metadata": {},
   "source": [
    "Next, we fit and transform the text data. Fitting the vectorizer on the text data creates the vocabulary, while the transform method transforms the text data into a BoW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd4988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = count_vectorizer.fit_transform(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f926c",
   "metadata": {},
   "source": [
    "The resulting `bow` object is a sparse matrix that represents the text data in BoW form. Go ahead and print the contents of this matrix to see how the words in the sample text have been mapped to their respective indices in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4bf6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 2 1 0 1 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 0 0 1 2 1 0 1 0 0 0 1 1 0]\n",
      " [1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0]\n",
      " [0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91649d6a",
   "metadata": {},
   "source": [
    "Each row in the matrix corresponds to a sentence in the vocabulary. The values in the matrix represent the frequency of each word in each sentence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb47ea",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The scikit-learn library provides a convenient implementation of the TF-IDF algorithm. \n",
    "1. First, run the cell below to import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46736703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2365b",
   "metadata": {},
   "source": [
    "2. Define the text that you want to analyze. This can be a list or array of strings, or even a Pandas DataFrame containing text data for a larger dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3619e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"Supervised learning is a type of machine learning.\",\n",
    "    \"Unsupervised learning is another type of machine learning.\",\n",
    "    \"Machine learning algorithms can be used for a wide range of applications, such as image recognition and natural language processing.\",\n",
    "    \"One of the most popular machine learning algorithms is the decision tree.\",\n",
    "    \"Random forest is an ensemble learning method that combines multiple decision trees.\",\n",
    "    \"Support vector machines are a powerful class of machine learning algorithms used for classification and regression tasks.\",\n",
    "    \"Neural networks are a type of machine learning algorithm inspired by the structure and function of the human brain.\",\n",
    "    \"Deep learning is a subfield of machine learning that involves neural networks with many layers.\",\n",
    "    \"Transfer learning is a technique in machine learning where a model trained on one task is used to improve performance on another related task.\",\n",
    "    \"Natural Language Processing (NLP) is a subfield of machine learning that deals with the interaction between computers and humans in natural language.\",\n",
    "    \"Text classification is an important task in NLP that involves assigning a document to one or more predefined categories.\",\n",
    "    \"Named Entity Recognition (NER) is another important task in NLP that involves identifying and extracting entities such as people, organizations, and locations from text data.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5cef4",
   "metadata": {},
   "source": [
    "3. Create an instance of the TfidfVectorizer class. You can specify any parameters you want to use. Some common parameters include:\n",
    "- `max_features`: the maximum number of features (unique words) to include in the TF-IDF matrix\n",
    "- `stop_words`: a list of words to exclude from the TF-IDF calculation, such as common stop words like \"the\" and \"and\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a6d3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f65fa",
   "metadata": {},
   "source": [
    "4. Fit and transform the text into a TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a89793",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615f05e",
   "metadata": {},
   "source": [
    "5. Print the resulting matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd233e02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.52146104 0.         0.28202368 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.64158678\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48673138 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.52146104 0.         0.28202368 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48673138 0.64158678 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.26082995 0.34381397 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34381397 0.         0.         0.         0.         0.\n",
      "  0.29527143 0.         0.13972045 0.         0.15113105 0.\n",
      "  0.         0.         0.         0.         0.29527143 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29527143 0.         0.34381397\n",
      "  0.29527143 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26082995\n",
      "  0.         0.34381397]\n",
      " [0.         0.39592654 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.44820694 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.21208851 0.         0.22940921 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.52189204 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.52189204 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35572313 0.         0.         0.\n",
      "  0.30549916 0.         0.         0.35572313 0.         0.\n",
      "  0.         0.35572313 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14456014 0.         0.         0.\n",
      "  0.35572313 0.         0.35572313 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.35572313 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35572313 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.24947888 0.         0.         0.         0.\n",
      "  0.32885151 0.28242149 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13363995 0.         0.14455396 0.32885151\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.32885151 0.         0.         0.         0.\n",
      "  0.         0.32885151 0.         0.         0.         0.\n",
      "  0.32885151 0.         0.32885151 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24947888\n",
      "  0.32885151 0.        ]\n",
      " [0.34484776 0.         0.         0.         0.34484776 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.34484776 0.34484776 0.         0.\n",
      "  0.         0.         0.         0.34484776 0.         0.\n",
      "  0.         0.         0.14014057 0.         0.15158547 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29615926 0.29615926 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34484776 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.26161422 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42100068 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31938664\n",
      "  0.         0.42100068 0.34217577 0.         0.18506018 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.36156028 0.36156028 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.36156028 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30526042 0.         0.         0.\n",
      "  0.         0.         0.24810583 0.         0.13418398 0.\n",
      "  0.         0.30526042 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.30526042\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30526042 0.         0.         0.\n",
      "  0.         0.46316362 0.         0.30526042 0.         0.30526042\n",
      "  0.30526042 0.         0.         0.         0.         0.23158181\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28502321 0.         0.28502321\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.28502321 0.\n",
      "  0.         0.         0.         0.         0.28502321 0.\n",
      "  0.48956249 0.         0.11582884 0.         0.12528827 0.\n",
      "  0.         0.         0.         0.         0.48956249 0.\n",
      "  0.         0.         0.21622911 0.         0.         0.\n",
      "  0.         0.         0.         0.24478124 0.         0.\n",
      "  0.         0.         0.         0.         0.24478124 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.35490322 0.         0.35490322\n",
      "  0.         0.30479501 0.         0.         0.         0.\n",
      "  0.         0.         0.35490322 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30479501 0.         0.         0.         0.26924266\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26924266 0.         0.         0.\n",
      "  0.         0.         0.35490322 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.26924266 0.         0.         0.30479501 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26784288 0.\n",
      "  0.         0.         0.         0.         0.26784288 0.26784288\n",
      "  0.26784288 0.         0.         0.         0.         0.26784288\n",
      "  0.         0.23002658 0.         0.         0.         0.20319549\n",
      "  0.         0.         0.         0.26784288 0.         0.\n",
      "  0.         0.         0.         0.26784288 0.         0.26784288\n",
      "  0.         0.         0.20319549 0.26784288 0.26784288 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.23002658 0.         0.         0.         0.         0.\n",
      "  0.         0.20319549 0.         0.         0.23002658 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59902e55",
   "metadata": {},
   "source": [
    "The resulting matrix is printed using the `toarray()` method, which converts the sparse matrix to a dense matrix for easier viewing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e18e2a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Word Embeddings\n",
    "\n",
    "Here, we will use the Gensim Python library to create word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd265ca",
   "metadata": {},
   "source": [
    "1. If it has not been installed, run the following line to install Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33b6084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Obtaining dependency information for FuzzyTM>=0.4.0 from https://files.pythonhosted.org/packages/2d/30/074bac7a25866a2807c1005c7852c0139ac22ba837871fc01f16df29b9dc/FuzzyTM-2.0.9-py3-none-any.whl.metadata\n",
      "  Using cached FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (2.0.3)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Obtaining dependency information for pyfume from https://files.pythonhosted.org/packages/ed/ea/a3b120e251145dcdb10777f2bc5f18b1496fd999d705a178c1b0ad947ce1/pyFUME-0.3.4-py3-none-any.whl.metadata\n",
      "  Using cached pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Obtaining dependency information for scipy>=1.7.0 from https://files.pythonhosted.org/packages/65/76/903324159e4a3566e518c558aeb21571d642f781d842d8dd0fd9c6b0645a/scipy-1.10.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached scipy-1.10.1-cp311-cp311-win_amd64.whl.metadata (58 kB)\n",
      "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Obtaining dependency information for simpful==2.12.0 from https://files.pythonhosted.org/packages/9d/0e/aebc2fb0b0f481994179b2ee2b8e6bbf0894d971594688c018375e7076ea/simpful-2.12.0-py3-none-any.whl.metadata\n",
      "  Using cached simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Using cached fst_pso-1.8.1-py3-none-any.whl\n",
      "Collecting pandas (from FuzzyTM>=0.4.0->gensim)\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/da/6d/1235da14daddaa6e47f74ba0c255358f0ce7a6ee05da8bf8eb49161aa6b5/pandas-1.5.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached pandas-1.5.3-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Using cached miniful-0.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Using cached FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
      "Using cached pyFUME-0.3.4-py3-none-any.whl (60 kB)\n",
      "Using cached scipy-1.10.1-cp311-cp311-win_amd64.whl (42.2 MB)\n",
      "Using cached pandas-1.5.3-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "Using cached simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: scipy, simpful, pandas, miniful, fst-pso, pyfume, FuzzyTM\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.1\n",
      "    Uninstalling scipy-1.11.1:\n",
      "      Successfully uninstalled scipy-1.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Hp\\\\anaconda3\\\\Lib\\\\site-packages\\\\~cipy\\\\fft\\\\_pocketfft\\\\pypocketfft.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3091de",
   "metadata": {},
   "source": [
    "2. Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0daf62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2dee7",
   "metadata": {},
   "source": [
    "3. Train a Word2Vec model using a tokenized text corpus. Word2Vec takes in tokens as its input, so the `word_tokenize` method from the previous quest is used here. We will make use of the `text_data` list from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94cefa42",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Hp/nltk_data'\n    - 'C:\\\\Users\\\\Hp\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Hp\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Hp\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m text_data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupervised learning is a type of machine learning.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupervised learning is another type of machine learning.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamed Entity Recognition (NER) is another important task in NLP that involves identifying and extracting entities such as people, organizations, and locations from text data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m ]\n\u001b[1;32m---> 16\u001b[0m tokens_in_text \u001b[38;5;241m=\u001b[39m [nltk\u001b[38;5;241m.\u001b[39mword_tokenize(sentence\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m text_data] \n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(tokens_in_text, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m text_data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupervised learning is a type of machine learning.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupervised learning is another type of machine learning.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamed Entity Recognition (NER) is another important task in NLP that involves identifying and extracting entities such as people, organizations, and locations from text data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m ]\n\u001b[1;32m---> 16\u001b[0m tokens_in_text \u001b[38;5;241m=\u001b[39m [nltk\u001b[38;5;241m.\u001b[39mword_tokenize(sentence\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m text_data] \n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(tokens_in_text, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Hp/nltk_data'\n    - 'C:\\\\Users\\\\Hp\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Hp\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Hp\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "text_data = [\n",
    "    \"Supervised learning is a type of machine learning.\",\n",
    "    \"Unsupervised learning is another type of machine learning.\",\n",
    "    \"Machine learning algorithms can be used for a wide range of applications, such as image recognition and natural language processing.\",\n",
    "    \"One of the most popular machine learning algorithms is the decision tree.\",\n",
    "    \"Random forest is an ensemble learning method that combines multiple decision trees.\",\n",
    "    \"Support vector machines are a powerful class of machine learning algorithms used for classification and regression tasks.\",\n",
    "    \"Neural networks are a type of machine learning algorithm inspired by the structure and function of the human brain.\",\n",
    "    \"Deep learning is a subfield of machine learning that involves neural networks with many layers.\",\n",
    "    \"Transfer learning is a technique in machine learning where a model trained on one task is used to improve performance on another related task.\",\n",
    "    \"Natural Language Processing (NLP) is a subfield of machine learning that deals with the interaction between computers and humans in natural language.\",\n",
    "    \"Text classification is an important task in NLP that involves assigning a document to one or more predefined categories.\",\n",
    "    \"Named Entity Recognition (NER) is another important task in NLP that involves identifying and extracting entities such as people, organizations, and locations from text data.\"\n",
    "]\n",
    "\n",
    "tokens_in_text = [nltk.word_tokenize(sentence.lower()) for sentence in text_data] \n",
    "\n",
    "model = Word2Vec(tokens_in_text, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b81db0",
   "metadata": {},
   "source": [
    "The cell above returns the list of sentences into individual tokens after converting the words to lowercase, and is passed into the Word2Vec model. \n",
    "\n",
    "The `min_count` parameter ignores all words with a total frequency lower than this. In this case, all words that only appear once is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d7921",
   "metadata": {},
   "source": [
    "4. Once your model is trained, you can access the word embeddings for any word like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['class']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5f80a",
   "metadata": {},
   "source": [
    "This will return a vector representing the word 'class' in the embedding space. The attribute `wv` of a Word2Vec model stands for \"word vector\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d69ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
